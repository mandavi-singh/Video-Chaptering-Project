# -*- coding: utf-8 -*-
"""Video_Chaptering_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k_UIQC6yKnnLx6e6rj6O0LrAEmt69YRL
"""

pip install google-api-python-client youtube-transcript-api pandas matplotlib scikit-learn

import re
import csv
import pandas as pd
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi

API_KEY = 'your_api_key_here'

def get_video_id(url):
    match = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    return match.group(1) if match else None

def get_video_title(video_id):
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    request = youtube.videos().list(part='snippet', id=video_id)
    response = request.execute()
    return response['items'][0]['snippet']['title'] if response['items'] else 'Unknown Title'

def get_video_transcript(video_id):
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        return transcript
    except Exception as e:
        print(f"Error: {e}")
        return []

def save_to_csv(title, transcript, filename):
    data = [{'start': entry['start'], 'text': entry['text']} for entry in transcript]
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    with open(filename, 'a', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Title:', title])

def main():
    url = input("Enter YouTube video link: ")
    video_id = get_video_id(url)
    if not video_id:
        print("Invalid URL")
        return
    title = get_video_title(video_id)
    transcript = get_video_transcript(video_id)
    if not transcript:
        print("Transcript not available")
        return
    filename = f"{video_id}_transcript.csv"
    save_to_csv(title, transcript, filename)
    print(f"Transcript saved to {filename}")

if __name__ == '__main__':
    main()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer

df = pd.read_csv("71op1DQ2gyo_transcript.csv")

# Remove rows with missing timestamps
df = df[pd.to_numeric(df['start'], errors='coerce').notnull()]
df['start'] = pd.to_numeric(df['start'])

# Add text length column
df['text_length'] = df['text'].apply(len)

# Plot distribution of text length
plt.hist(df['text_length'], bins=50, color='skyblue')
plt.title("Distribution of Text Lengths")
plt.xlabel("Text Length")
plt.ylabel("Frequency")
plt.show()

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import NMF

vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
X = vectorizer.fit_transform(df['text'])

nmf = NMF(n_components=5, random_state=1).fit(X)
feature_names = vectorizer.get_feature_names_out()

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\nTopic {topic_idx + 1}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

display_topics(nmf, feature_names, 10)

from youtube_transcript_api import YouTubeTranscriptApi

# Use video ID
video_id = "V_0Kvr9Mo-8"

# Get the Hindi auto-generated transcript
transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['hi'])

# Display transcript
for entry in transcript:
    print(entry['text'])

from youtube_transcript_api import YouTubeTranscriptApi

video_id = "V_0Kvr9Mo-8"
transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['hi'])
hindi_text = " ".join([entry['text'] for entry in transcript])

hindi_stopwords = [
    'के', 'का', 'की', 'से', 'है', 'था', 'थे', 'पर', 'को', 'में', 'और', 'ने',
    'यह', 'जो', 'भी', 'तो', 'तक', 'अब', 'या', 'अगर', 'लेकिन', 'कभी', 'बहुत',
    'कर', 'करना', 'किया', 'रहा', 'रहे', 'रही', 'हुआ', 'हुई'
]

vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=hindi_stopwords)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import NMF

# Hindi stopwords
hindi_stopwords = [
    'के', 'का', 'की', 'से', 'है', 'था', 'थे', 'पर', 'को', 'में', 'और', 'ने',
    'यह', 'जो', 'भी', 'तो', 'तक', 'अब', 'या', 'अगर', 'लेकिन', 'कभी', 'बहुत',
    'कर', 'करना', 'किया', 'रहा', 'रहे', 'रही', 'हुआ', 'हुई'
]

# Corrected corpus (as string list)
corpus = [
    "अब इस जगह में कोई सुकून महसूस नहीं हो रहा हम तो जिंदा है मगर इसको कहते हैं जिंदा मुर्दा",
    "हमारा जो अभी खाने का चावल उबल रहा था अब खाने के लिए तैयार हो रहा था अब हम खाने आने वाले थे"
]

# Vectorization
vectorizer = CountVectorizer(max_df=0.95, min_df=1, stop_words=hindi_stopwords)
X = vectorizer.fit_transform(corpus)

# Topic Modeling
nmf_model = NMF(n_components=2, random_state=42)
nmf_model.fit(X)

# Display topics
for index, topic in enumerate(nmf_model.components_):
    print(f"Topic {index + 1}:")
    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]])

doc_topic_matrix = nmf_model.transform(X)

for i, topic_dist in enumerate(doc_topic_matrix):
    dominant_topic = topic_dist.argmax()
    print(f"Document {i+1} is mostly about Topic {dominant_topic + 1}")

import pandas as pd

topic_keywords = []

for index, topic in enumerate(nmf_model.components_):
    top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]
    topic_keywords.append({"Topic": f"Topic {index + 1}", "Keywords": ", ".join(top_words)})

df = pd.DataFrame(topic_keywords)
df.to_csv("hindi_topics.csv", index=False)

from youtube_transcript_api import YouTubeTranscriptApi

video_id = "pykB_fncbkE"

# Request the transcript in Hindi
transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['hi'])

for entry in transcript:
    print(entry['text'])

from youtube_transcript_api import YouTubeTranscriptApi
from googletrans import Translator

#Get Hindi transcript
video_id = "pykB_fncbkE"
transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['hi'])

#Translate to English
translator = Translator()

for entry in transcript[:15]:
    hindi_text = entry['text']
    translated = translator.translate(hindi_text, src='hi', dest='en')
    print(f"Hindi: {hindi_text}")
    print(f"English: {translated.text}")
    print("-" * 50)

pip install youtube-transcript-api googletrans==4.0.0-rc1

from youtube_transcript_api import YouTubeTranscriptApi
from googletrans import Translator

#Get Hindi transcript
video_id = "pykB_fncbkE"
transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['hi'])

#Translate only first 10 lines
translator = Translator()

for entry in transcript[:10]:
    hindi_text = entry['text']
    translated = translator.translate(hindi_text, src='hi', dest='en')
    print(f"Hindi: {hindi_text}")
    print(f"English: {translated.text}")
    print("-" * 50)

import csv
from youtube_transcript_api import YouTubeTranscriptApi
from googletrans import Translator

# Step 1: Get Hindi transcript
video_id = "pykB_fncbkE"
transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['hi'])

# Step 2: Initialize translator
translator = Translator()

# Step 3: Prepare data (limit to first 10 lines)
translated_data = []
for entry in transcript[:10]:  # change number of lines here
    hindi_text = entry['text']
    translated = translator.translate(hindi_text, src='hi', dest='en')
    translated_data.append([hindi_text, translated.text])

# Step 4: Save to CSV
with open('translated_subtitles.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Hindi', 'English'])  # Header
    writer.writerows(translated_data)

print("✅ CSV file 'translated_subtitles.csv' saved successfully!")

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Read the CSV
df = pd.read_csv('translated_subtitles.csv')

# Step 2: Add a column with sentence length (number of characters)
df['Hindi Length'] = df['Hindi'].str.len()
df['English Length'] = df['English'].str.len()

# Step 3: Plot bar chart comparing lengths
plt.figure(figsize=(10, 5))
plt.bar(range(len(df)), df['Hindi Length'], label='Hindi', alpha=0.7)
plt.bar(range(len(df)), df['English Length'], label='English', alpha=0.7)
plt.xlabel('Line Number')
plt.ylabel('Sentence Length (characters)')
plt.title('Sentence Length Comparison (Hindi vs English)')
plt.legend()
plt.tight_layout()
plt.show()

from collections import Counter
import re

# Helper function to clean text
def clean_text(text):
    return re.sub(r'\W+', ' ', text.lower())

# Clean and tokenize the text
hindi_words = " ".join(df['Hindi'].apply(clean_text)).split()
english_words = " ".join(df['English'].apply(clean_text)).split()

# Get the most common words
hindi_word_count = Counter(hindi_words).most_common(10)
english_word_count = Counter(english_words).most_common(10)

# Print the most common words
print("Most common Hindi words:", hindi_word_count)
print("Most common English words:", english_word_count)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Create a word cloud for Hindi
hindi_text = " ".join(df['Hindi'])
hindi_wordcloud = WordCloud(width=800, height=400).generate(hindi_text)

# Create a word cloud for English
english_text = " ".join(df['English'])
english_wordcloud = WordCloud(width=800, height=400).generate(english_text)

# Plot both word clouds
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.imshow(hindi_wordcloud, interpolation='bilinear')
plt.title("Hindi Word Cloud")
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(english_wordcloud, interpolation='bilinear')
plt.title("English Word Cloud")
plt.axis('off')

plt.show()

import matplotlib.pyplot as plt

# Calculate the sentence lengths for both Hindi and English
df['Hindi_Length'] = df['Hindi'].apply(len)
df['English_Length'] = df['English'].apply(len)

# Plot the sentence lengths over time
plt.figure(figsize=(10, 6))

plt.plot(df.index, df['Hindi_Length'], label='Hindi Sentence Length')
plt.plot(df.index, df['English_Length'], label='English Sentence Length')

plt.xlabel('Time (Index)')
plt.ylabel('Sentence Length')
plt.title('Sentence Lengths Over Time')
plt.legend()

plt.show()

from textblob import TextBlob

# Helper function to get sentiment
def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

# Apply sentiment analysis to both Hindi and English
df['Hindi_Sentiment'] = df['Hindi'].apply(get_sentiment)
df['English_Sentiment'] = df['English'].apply(get_sentiment)

# Plot the sentiment analysis over time
plt.figure(figsize=(10, 6))

plt.plot(df.index, df['Hindi_Sentiment'], label='Hindi Sentiment', color='blue')
plt.plot(df.index, df['English_Sentiment'], label='English Sentiment', color='red')

plt.xlabel('Time (Index)')
plt.ylabel('Sentiment')
plt.title('Sentiment Analysis Over Time')
plt.legend()

plt.show()

print(df.columns)

# Step 1: Vectorize the 'English' column for NMF
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
X = vectorizer.fit_transform(df['English'])  # Use 'English' column instead of 'text'

# Step 2: Apply NMF and display topics as usual
nmf = NMF(n_components=5, random_state=1).fit(X)
feature_names = vectorizer.get_feature_names_out()

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\nTopic {topic_idx + 1}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

display_topics(nmf, feature_names, 10)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import NMF

# Step 1: Vectorize the 'Hindi' column for NMF
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=None)  # Set stop_words to None
X = vectorizer.fit_transform(df['Hindi'])  # Use 'Hindi' column for the text data

# Step 2: Apply NMF and display topics as usual
nmf = NMF(n_components=5, random_state=1).fit(X)
feature_names = vectorizer.get_feature_names_out()

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\nTopic {topic_idx + 1}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

display_topics(nmf, feature_names, 10)

import re

def clean_hindi(text):
    # Remove special characters and digits
    text = re.sub(r'[^\u0900-\u097F\s]', '', text)  # Keep only Hindi chars
    text = re.sub(r'\d+', '', text)  # Remove digits
    text = re.sub(r'\s+', ' ', text).strip()  # Normalize whitespace
    return text

df['Hindi_cleaned'] = df['Hindi'].apply(clean_hindi)

nmf = NMF(n_components=5, random_state=1).fit(X)
feature_names = vectorizer.get_feature_names_out()

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\nTopic {topic_idx + 1}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

display_topics(nmf, feature_names, 10)

# Define a simple list of Hindi stopwords
hindi_stopwords = [
    'के', 'का', 'की', 'से', 'है', 'और', 'को', 'पर', 'यह', 'था', 'हैं', 'तो', 'में',
    'कि', 'जो', 'भी', 'कर', 'करना', 'किया', 'हो', 'रहा', 'रहे', 'एक', 'लेकिन', 'क्या',
    'जैसे', 'जब', 'तक', 'लिए', 'अब', 'या', 'नहीं', 'सकता', 'सकते', 'गया', 'दी', 'था',
    'थी', 'थे', 'होता', 'होती', 'होते', 'अपने', 'आप', 'हम', 'आपका', 'मेरे', 'उस', 'उन'
]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import NMF

# Vectorization
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=hindi_stopwords)
X = vectorizer.fit_transform(df['Hindi'])  # or df['Hindi_cleaned'] if cleaned

# Apply NMF
nmf = NMF(n_components=5, random_state=1)
nmf.fit(X)

# Get feature names
feature_names = vectorizer.get_feature_names_out()

# Display topics
def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\nTopic {topic_idx + 1}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

display_topics(nmf, feature_names, 10)